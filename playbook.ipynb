{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 7810"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report_date - дата среза фичей\n",
    "\n",
    "есть есть еденица мы не знаем какая она 1 и 2 или 3е"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change filenames here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('train_dataset_Самолет.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text fields\n",
    "colomns #\n",
    "\n",
    "URL: 49-56 221-244 457-464 545-560 1045-1076\n",
    "\n",
    "От куда перешли: 555 - 576\n",
    "\n",
    "Поисковые запросы: 577 - 584\n",
    "\n",
    "Ипотека: 129-152\n",
    "\n",
    "Платформа 521-544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_text_col = ['col49','col50','col51','col52','col53','col54','col55','col56',\n",
    "                            'col221','col222','col223','col224','col225','col226','col227','col228','col229','col230','col231','col232','col233','col234','col235','col236','col237','col238','col239','col240','col241','col242','col243','col244',\n",
    "                            'col457','col458','col459','col460','col461','col462','col463','col464',\n",
    "                            'col545','col546','col547','col548','col549','col550','col551','col552','col553','col554','col555','col556','col557','col558','col559','col560',\n",
    "                            'col1045','col1046','col1047','col1048','col1049','col1050','col1051','col1052','col1053','col1054','col1055', 'col1056',\n",
    "                            'col1057','col1058','col1059','col1060','col1061','col1062','col1063','col1064','col1065','col1066','col1067','col1068','col1069','col1070','col1071','col1072','col1073','col1074','col1075','col1076',\n",
    "                            'col561','col562','col563','col564','col565','col566','col567','col568','col569','col570','col571','col572','col573','col574','col575','col576',\n",
    "                            'col577','col578','col579','col580','col581','col582','col583','col584',\n",
    "                            'col129','col130','col131','col132','col133','col134','col135','col136','col137','col138','col139','col140','col141','col142','col143','col144','col145','col146','col147','col148','col149','col150','col151','col152',\n",
    "                            'col521','col522','col523','col524','col525','col526','col527','col528','col529','col530','col531','col532','col533','col534','col535','col536','col537','col538','col539','col540','col541','col542','col543','col544']\n",
    "len(list_of_text_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_features(raw_data):\n",
    "    EF = raw_data\n",
    "    #EF = raw_data.copy()\n",
    "    EF[list_of_text_col] = EF[list_of_text_col].astype(str)\n",
    "    EF['text_for_search'] = EF[list_of_text_col].agg(', '.join, axis=1)\n",
    "    import re\n",
    "    ipoteka = ['Ипоте', 'Ipote','mortgag']\n",
    "    comercial = ['comercial']\n",
    "    ads = ['utm_source']\n",
    "    family = ['family', 'Детска']\n",
    "    parking = ['parki']\n",
    "    apple = ['Mac', 'ios', 'iPhone', 'iPad', \"Safari\"]\n",
    "    nonapple = ['Android', 'Windows', 'Linux']\n",
    "    flats = ['flats']\n",
    "    price_regex_min = 'price_min=\\d+'\n",
    "    price_regex_max = 'price_max=\\d+'\n",
    "\n",
    "    EF['ipoteka'] = 0\n",
    "    EF['comercial'] = 0\n",
    "    EF['ads'] = 0\n",
    "    EF['family'] = 0\n",
    "    EF['parking'] = 0\n",
    "    EF['apple'] = 0\n",
    "    EF['nonapple'] = 0\n",
    "    EF['flats']=0\n",
    "    EF['price_min'] = 0\n",
    "    EF['price_max'] = 0\n",
    "    EF['price_mid'] = 0\n",
    "\n",
    "    for index, row in EF.iterrows():\n",
    "        count = 0\n",
    "\n",
    "        for w in ipoteka:\n",
    "            if re.search(w, row['text_for_search'], re.IGNORECASE):\n",
    "                count+=1\n",
    "        EF.at[index, 'ipoteka'] = count\n",
    "        count = 0\n",
    "\n",
    "        for w in flats:\n",
    "            if re.search(w, row['text_for_search'], re.IGNORECASE):\n",
    "                count+=1\n",
    "        EF.at[index, 'flats'] = count\n",
    "        count = 0\n",
    "\n",
    "        for w in apple:\n",
    "            if re.search(w, row['text_for_search'], re.IGNORECASE):\n",
    "                count+=1\n",
    "        EF.at[index, 'apple'] = count\n",
    "        count = 0\n",
    "\n",
    "        for w in nonapple:\n",
    "            if re.search(w, row['text_for_search'], re.IGNORECASE):\n",
    "                count+=1\n",
    "        EF.at[index, 'nonapple'] = count\n",
    "        count = 0\n",
    "\n",
    "        for w in comercial:\n",
    "            if re.search(w, row['text_for_search'], re.IGNORECASE):\n",
    "                count+=1\n",
    "        EF.at[index, 'comercial'] = count\n",
    "        count = 0\n",
    "\n",
    "        for w in ads:\n",
    "            if re.search(w, row['text_for_search'], re.IGNORECASE):\n",
    "                count+=1\n",
    "        EF.at[index, 'ads'] = count\n",
    "        count = 0\n",
    "\n",
    "        for w in family:\n",
    "            if re.search(w, row['text_for_search'], re.IGNORECASE):\n",
    "                count+=1\n",
    "        EF.at[index, 'family'] = count\n",
    "        count = 0\n",
    "\n",
    "        for w in parking:\n",
    "            if re.search(w, row['text_for_search'], re.IGNORECASE):\n",
    "                count+=1\n",
    "        EF.at[index, 'parking'] = count\n",
    "        count = 0\n",
    "        min_p = None\n",
    "        max_p = None\n",
    "        min_p = re.search(price_regex_min, row['text_for_search'])\n",
    "        max_p = re.search(price_regex_max, row['text_for_search'])\n",
    "    \n",
    "\n",
    "        if min_p != None:\n",
    "            min_p = min_p.group(0)[10:]\n",
    "            #print(int(min_p))\n",
    "            EF.at[index, 'price_min'] = int(min_p)\n",
    "        if max_p != None:\n",
    "            max_p = max_p.group(0)[10:]\n",
    "            #print(int(max_p))\n",
    "            EF.at[index, 'price_max'] = int(max_p)   \n",
    "        if min_p != None and max_p != None:\n",
    "            EF.at[index, 'price_mid']=(int(min_p)+int(max_p))/2\n",
    "    EF = EF.drop(columns=list_of_text_col)\n",
    "    EF = EF.drop(columns=['text_for_search'])\n",
    "    return EF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = extra_features(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['price_min'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['price_max'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['price_mid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['flats'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['apple'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['nonapple'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['ipoteka'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['ads'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['family'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['parking'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unq_clients = raw_data['client_id'].unique().shape[0] # Number of unque clients\n",
    "num_unq_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.drop(columns=['report_date','client_id'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление столбцов с хеш-суммами, uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regexp для поиска и удаления хеш сумм, так как это уникальное значение\n",
    "pattern = r'^[a-zA-Z0-9]{8}-[a-zA-Z0-9]{4}-[a-zA-Z0-9]{4}-[a-zA-Z0-9]{4}-[a-zA-Z0-9]{12}$'\n",
    "\n",
    "# Функция для фильтрации колонок по регулярному выражению\n",
    "def filter_columns(column_name):\n",
    "    return not data[column_name].astype(str).str.match(pattern).any()\n",
    "\n",
    "raw_data_wout_regex = data[data.columns[data.columns.to_series().apply(filter_columns)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat = raw_data_wout_regex.copy()\n",
    "datat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on train test raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(datat.drop(columns=['target']), datat['target'], test_size=0.01, random_state=rs+20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data = X_train\n",
    "Train_data.insert(loc=0, column='target', value=y_train)\n",
    "Test_data = X_test\n",
    "Test_data.insert(loc=0, column='target', value=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление строк с целевым признаком 0, у которых большинство (медианное значение) признаков Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Надо посчитать сколько в среднем заполнено значений для цп 0\n",
    "median_t0 = Train_data[Train_data['target'] == 0].iloc[:,1:].apply(lambda row: row.notna().sum(), axis=1).median()\n",
    "median_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Train_data[~((Train_data['target'] == 0) & (Train_data.iloc[:, 1:].apply(lambda row: row.notna().sum(), axis=1) <= median_t0))]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['target']\n",
    "data = data.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проход окном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дропаем колонки в которых только NaN\n",
    "good_data = data.drop(data.columns[data.isna().all()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Серднее окном в 2 колонки\n",
    "mean_gd = (good_data.count() + good_data.count().shift(-1) + good_data.count().shift(1))/3\n",
    "mean_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selcol = good_data.columns[good_data.count()>=mean_gd] ########### > || >= фичи х2 ~1300\n",
    "selcol = selcol.to_list()\n",
    "selcol.append(data.columns[-1]) # добавляем последнюю колонку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data = good_data[selcol]\n",
    "good_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Колонки с категориальными данными\n",
    "catCols = [col for col in good_data.columns if good_data[col].dtype==\"object\"]\n",
    "catCols[-10:-1]\n",
    "#print(f'number cat columns = {len(catCols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = good_data[catCols]\n",
    "numeric_columns = good_data.drop(catCols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация данных\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "transform_gd = scaler.fit_transform(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_target = Test_data['target']\n",
    "Test_data2 = Test_data[numeric_columns.columns]\n",
    "Test_data3 = scaler.transform(Test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistGradientBoostingClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open (\"transform_gd.pkl\", 'wb') as fff:\n",
    "    pickle.dump(transform_gd, file=fff)\n",
    "with open (\"target.pkl\", 'wb') as fff:\n",
    "    pickle.dump(target, file=fff)\n",
    "\n",
    "with open (\"Test_target.pkl\", 'wb') as fff:\n",
    "    pickle.dump(Test_target, file=fff)\n",
    "with open (\"Test_data3.pkl\", 'wb') as fff:\n",
    "    pickle.dump(Test_data3, file=fff)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"transform_gd.pkl\", 'rb') as fff:\n",
    "    transform_gd =  pickle.load(file=fff)\n",
    "with open (\"target.pkl\", 'rb') as fff:\n",
    "    target = pickle.load(file=fff)\n",
    "\n",
    "with open (\"Test_target.pkl\", 'rb') as fff:\n",
    "    Test_target = pickle.load(file=fff)\n",
    "with open (\"Test_data3.pkl\", 'rb') as fff:\n",
    "    Test_data3 = pickle.load(file=fff) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "if False:\n",
    "#parameters to tune\n",
    "    parameters = {\n",
    " 'max_iter': [150, 175, 200, 225, 250, 300, 350, 400],\n",
    " 'learning_rate': [0.1],\n",
    " 'min_samples_leaf': [1, 3, 5, 7],\n",
    " 'l2_regularization': [0.05, 0.03, 0.07],\n",
    " 'max_leaf_nodes': [7, 10, 13, 15],\n",
    " 'max_bins': [190, 200, 210],\n",
    " 'scoring': ['roc_auc'],\n",
    " 'random_state' : [rs+20],\n",
    "    }\n",
    "#instantiate the gridsearch\n",
    "    hgb_grid = GridSearchCV(HistGradientBoostingClassifier(), parameters, n_jobs=-1, \n",
    "    cv=5, scoring='roc_auc',\n",
    "    verbose=3, refit=True)\n",
    "#fit on the grid \n",
    "    hgb_grid.fit(transform_gd,target)\n",
    "    import pickle\n",
    "    with open (\"hgb_grid.pkl\", 'wb') as fff:\n",
    "        pickle.dump(hgb_grid, file=fff)\n",
    "    with open (\"hgb_grid.pkl\", 'rb') as fff:\n",
    "        hgb_grid = pickle.load(file=fff)\n",
    "    print(hgb_grid.best_params_)\n",
    "    print(\"Test: \",roc_auc_score(Test_target, hgb_grid.best_estimator_.predict(Test_data3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gbc= HistGradientBoostingClassifier(loss='binary_crossentropy', \n",
    "                                    max_iter=450,\n",
    "                                    l2_regularization=0.05,\n",
    "                                    max_leaf_nodes=15,\n",
    "                                    max_depth = None,\n",
    "                                    min_samples_leaf = 7, \n",
    "                                    max_bins = 200,\n",
    "                                    learning_rate = 0.01,  \n",
    "                                    random_state=rs+20)\n",
    "gbc.fit(transform_gd,target)\n",
    "\n",
    "print(\"Train: \", roc_auc_score(target, gbc.predict(transform_gd)))\n",
    "print(\"Test: \",roc_auc_score(Test_target, gbc.predict(Test_data3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change filenames here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = extra_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target2 = test_data[numeric_columns.columns]\n",
    "test_target3 = scaler.transform(test_target2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gbc.predict_proba(test_target3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1=0\n",
    "count2=0\n",
    "for i, r in pd.DataFrame(pred).iterrows():\n",
    "    if r[1]<0.5:\n",
    "        count1+=1\n",
    "    else: \n",
    "        count2+=1\n",
    "print(count1, count2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change filenames here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submission.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target']=pd.DataFrame(pred)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change filenames here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
